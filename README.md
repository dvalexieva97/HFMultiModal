# ðŸ¤—Hugging Face ML researcher/engineer code exercise - Funky multi-modal version
## Dianka Alexieva

*Below is a summary of the process, notes, potential for improvements for the Hugging Face ML researcher coding 
exercise.*

This repo contains the following folders and files:
- ./data - folder with generated dataset:
  1. *dataset_dic.pkl* - a pickle with our generated text dataset per ImangeNet class
  2. *input_texts.txt* - a .txt file with all input texts
  3. *labels_tensor.bin* - a torch tensor with ImageNet labels for our dataset
  4. *tokens_tensor.bin* - a torch tensor with our tokenized model input sequences
- *BERT_sentence2ImageNet.ipynb* - notebook with fine-tuning of BERT sequence classification
- ./model - folder with fine-tuned sequence classification model
- *prepare_data.py* - script to generate data
- *run_model* - utilities to generate an image from text
- *demo.py* - a demo script generating images from a list of sentences
- *sample_images* - folder with savede sample images from sample sentences


### Building a dataset of text sequence associated to ImageNet classes

### Workflow:
To generate a dataset of sentences associated to ImageNet classes (for the 598 simple classes filtered from the original 
1K), I used multiple text generation methods for each ImageNet class:

1. Multiple sentence generation models used:
- WordNet example sentences fetched for each ImageNet class (when available)
- GPT-2 sentence generation from individual classes using beam search decoding to get most likely output sequences
- Sentence generation using k2t (key2text model) (also with beam search)
- Simple sentence pattern (in case no relevant sentences were generated by the above models)
2. The generated sentences were filtered for Part of Speech, as some original generated sentences changed the POS 
   (e.g. bear --> bearing) 
The generated sequences were tokenized into sentences and were filtered to return only sentences containing the 
   target word.
    
**Execution time for 598 simple classes:** ~2 hours

**Dataset size:** 3940 sentences

### Improvements:
Originally I wanted to create a multi-label multiclass model. 
For this, I wanted to generate sentences using combinations of multiple ImageNet classes. 
However, a binomial combination of our 598 classes would yield ~500K combinations, which wasn't feasible for this 
exercise.
As an solution to this, I would like to use a word similarity filter (e.g. simple cosine, or using BERT) to define 
the most relevant seed word combinations and generate sentences with those combinations.

Furthermore, using the current sentence generation methods, playing around with the max sequence length, as well as the 
beam parameters could yield even more relevant sentences.

Finally, I used nltk's WordNet implementation for POS tagging, however I noticed that it does not always work perfectly.
A fine-tuned transformer for POS tagging could be used to improve sentence filtering.

### Building a dataset for training a mapping function

### Workflow:
I generated the training dataset using the default function in prepare_data.py
As proposed in the task, I 
- Generated input vectors from the outputs of a pretrained language model BERT for our examples - 
  as the input sentences were different lengths, I added paddinng to up to 30 tokens 
  (assuming model inputs are short sentences)
- Generated target vectors. Here instead of extracting associated target class embeddings from BigGAN's input embedding 
  matrix, I saved the class ids as a torch tensor

Since the generated sequences were of different lengths, I added padding to the input vectors.

**folder where dataset is saved:** './data'

### Improvements:

1. As an improvement, I would also generate an attention_mask vector for each input vector to improve model training.
2. As mentioned previously, would generate a multi-label dataset and train the model on it, in order to 
3. I would also generated longer text sequences, so that we can generate meaningful results from larger pieces of input 
   text. 
4. As can be seen in demo.py, abstract sentences or sentences which do not contain any ImageNet classes generate 
   unpredictable output. To the sequence generation training I would add sentences which do not contain any of our ImageNEt classes and would 
   yield empty images for such input.
5. Similarly to p.4, I would generate non-class sentences (e.g. "I do not see a cat") to generate empty images for such 
   cases.
5. Finally, I would work on speeding up the data generation process, as it is definitely too long at the moment.

### Learning a mapping function

### Workflow

For the mapping function, I did the following:

Rather than training BERT's embedding space to BigGAN's, I fine-tuned BERT for sequence classification using our 
dataset.
I then mapped back the output of the fine-tuned sequence classification model to ImageNet's class and 
generated ImageNet's (,128) embedding from the class.

For this, I tweaked the text_to_image() function in run_model.py
The mapping_model() function I created works as follows:
1. It takes a text sequence as input and tokenizes it 
2. The tokenized input is run through the fine-tuned sequence classification model
3. The most likely predicted output of the classification model is converted to an ImageNet class (id) 
4. From the class id I generated a one_hot_label vector and then a BigGAN ImageNet embedding: mapping_output.

    The function returns the mapping_output, as well as the token vector, needed to generate the noise_seed_vector for the 
   final image.

**Sequence classification Model Parameters:**

- batch_size = 32
- epochs = 30
- optimizer = AdamW


### Improvements:
As an improvement, I would train a multi-label multiclass model based on combinations of ImageNet classes 
  ("The cat and dog are playing." --> ["cat", "dog"]). 
   
The output of the multi-label multiclass model I would map to ImageNet's classes and then to generate an image using 
interpolations (something similar to this 
[excellent notebook](https://colab.research.google.com/drive/1MhfEAOBwhGu1A-F2NSVxGQrkJ4vk7w4V#scrollTo=dSAyfDfnVugs&forceEdit=true&offline=true&sandboxMode=true))

### Note: 
I realize I did not solve the task exactly as requested. My reasoning behind this was that at the end of the day our 
input for GAN was a ImageNet class (or for multi-label: a set of classes). Thus, as we do not have additional GAN 
inputs apart from the ImageNet classes (e.g. embeddings about the movement of an image:
"a cat sitting" vs. "a cat jumping" ). To me this meant we can simplify the task to a sequence classification task with 
direct linear mapping between the sequence classification output layer and BigGAN's embedding. 

### Alternatives

I do however see the value of latent space mapping and did some research on it. 

1. The mapping model could be a variational autoencoder translating BERT's hidden states to BigGAN's input 
   ([Theodoridis et al., 2020](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w56/Theodoridis_Cross-Modal_Variational_Alignment_of_Latent_Spaces_CVPRW_2020_paper.pdf)). 
2. Another solution to latent space mapping has been described by 
   [Skorokhodov et al., 2021](https://arxiv.org/abs/2104.06954). The authors propose a GAN-based generator in an 
   adversarial setting. 


### Demo

demo.py can be found generating images from a list of sample sentences (not included in the original dataset)


